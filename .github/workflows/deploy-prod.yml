name: deploy-prod

on:
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Container tag/digest to deploy"
        required: true
        type: string

permissions:
  id-token: write
  contents: read
  packages: read

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v4
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE_ARN }}
          aws-region: eu-west-2
      - name: Capture previous task definition
        id: prev
        env:
          ECS_CLUSTER: ${{ vars.ECS_CLUSTER }}
          ECS_SERVICE: ${{ vars.ECS_SERVICE }}
        run: |
          PREV_TASK_DEF=$(aws ecs describe-services \
            --cluster "$ECS_CLUSTER" \
            --services "$ECS_SERVICE" \
            --query 'services[0].taskDefinition' \
            --output text)
          echo "task_def=$PREV_TASK_DEF" >> "$GITHUB_OUTPUT"
      - name: Canary deployment (10%)
        env:
          ECS_CLUSTER: ${{ vars.ECS_CLUSTER }}
          ECS_SERVICE: ${{ vars.ECS_SERVICE }}
        run: |
          test -n "$ECS_CLUSTER"
          test -n "$ECS_SERVICE"
          aws ecs update-service \
            --cluster "$ECS_CLUSTER" \
            --service "$ECS_SERVICE" \
            --force-new-deployment
      - name: Monitor canary window
        id: monitor
        env:
          SLO_ERROR_ALARM: ${{ vars.SLO_ERROR_ALARM }}
          SLO_P95_ALARM: ${{ vars.SLO_P95_ALARM }}
        run: |
          set +e
          breached=false
          for i in {1..15}; do
            ERROR_STATE=$(aws cloudwatch describe-alarms --alarm-names "$SLO_ERROR_ALARM" --query 'MetricAlarms[0].StateValue' --output text)
            P95_STATE=$(aws cloudwatch describe-alarms --alarm-names "$SLO_P95_ALARM" --query 'MetricAlarms[0].StateValue' --output text)
            if [ "$ERROR_STATE" = "ALARM" ] || [ "$P95_STATE" = "ALARM" ]; then
              breached=true
              break
            fi
            sleep 60
          done
          echo "breached=$breached" >> "$GITHUB_OUTPUT"
          if [ "$breached" = "true" ]; then
            exit 1
          fi
      - name: Promote to full traffic
        if: ${{ success() }}
        env:
          ECS_CLUSTER: ${{ vars.ECS_CLUSTER }}
          ECS_SERVICE: ${{ vars.ECS_SERVICE }}
        run: |
          aws ecs update-service \
            --cluster "$ECS_CLUSTER" \
            --service "$ECS_SERVICE" \
            --force-new-deployment
          aws ecs wait services-stable \
            --cluster "$ECS_CLUSTER" \
            --services "$ECS_SERVICE"
      - name: Post release marker
        if: ${{ success() && vars.RELEASE_MARKER_ENDPOINT != '' }}
        env:
          RELEASE_MARKER_ENDPOINT: ${{ vars.RELEASE_MARKER_ENDPOINT }}
          RELEASE_MARKER_TOKEN: ${{ secrets.RELEASE_MARKER_TOKEN }}
          RELEASE_MARKER_ORG_ID: ${{ vars.RELEASE_MARKER_ORG_ID }}
          RELEASE_ENVIRONMENT: production
          RELEASE_VERSION: ${{ vars.RELEASE_VERSION }}
          RELEASE_COMMIT_SHA: ${{ github.sha }}
          RELEASE_IMAGE_DIGEST: ${{ github.event.inputs.image_tag }}
          RELEASE_MIGRATION_VERSION: ${{ vars.RELEASE_MIGRATION_VERSION }}
        run: node autonomy/scripts/postReleaseMarker.mjs
      - name: Auto rollback on SLO breach
        if: ${{ failure() }}
        env:
          ECS_CLUSTER: ${{ vars.ECS_CLUSTER }}
          ECS_SERVICE: ${{ vars.ECS_SERVICE }}
          PREVIOUS_TASK_DEF: ${{ steps.prev.outputs.task_def }}
        run: |
          aws ecs update-service \
            --cluster "$ECS_CLUSTER" \
            --service "$ECS_SERVICE" \
            --task-definition "$PREVIOUS_TASK_DEF" \
            --force-new-deployment
          aws ecs wait services-stable \
            --cluster "$ECS_CLUSTER" \
            --services "$ECS_SERVICE"
